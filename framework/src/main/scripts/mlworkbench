#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright (C) 2016  EkStep Foundation
# 
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
# 
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
# 
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

import os
import shutil
import sys
import yaml
from mlworkbench.utils.common import normalize_path, create_directory, get_parent_dir

def getopts(argv):
    opts = {}  # Empty dictionary to store key-value pairs.
    while argv:  # While there are arguments left to parse...
        if argv[0][0] == '-':  # Found a "-name value" pair.
            opts[argv[0]] = argv[1]  # Add key and value to the dictionary.
        # Reduce the argument list by copying it starting from index 1.
        argv = argv[1:]
    return opts

def setup_exec_environment(vargs):

    if "MLWB_HOME" in os.environ:
        working_dir = os.environ["MLWB_HOME"]
    else:
        working_dir = os.environ['HOME'] + "/MLWB_HOME"

    create_directory(working_dir + '/airflow')
    create_directory(working_dir + '/airflow/dags')
    create_directory(working_dir + '/mlwb_dags')

    os.environ['AIRFLOW_HOME'] = working_dir + '/airflow'
    os.environ['AIRFLOW__CORE__DAGS_FOLDER'] = working_dir + '/airflow/dags'
    os.environ['AIRFLOW__CORE__LOAD_EXAMPLES'] = 'False'
    os.environ['DAG_FOLDER'] = working_dir + '/mlwb_dags'

def get_dag_state(dag_id):
    from airflow.models import DagRun
    dr = DagRun.find(dag_id)
    return dr[0].state if len(dr) > 0 else None

def init():
    
    if "MLWB_HOME" in os.environ:
        working_dir = os.environ["MLWB_HOME"]
    else:
        working_dir = os.environ['HOME'] + "/MLWB_HOME"

    create_directory(working_dir + '/airflow')

    if os.path.isdir(working_dir + '/airflow/dags/'):
        shutil.rmtree(working_dir + '/airflow/dags/')
    os.makedirs(working_dir + '/airflow/dags/')

    pkgdir = sys.modules['mlworkbench'].__path__[0]
    airflow_dag_file = pkgdir + '/executor/airflow_executor/dags/airflow_dag.py'
    
    shutil.copy2(airflow_dag_file, working_dir + '/airflow/dags')

def run(vargs, execution_dir):

    if "MLWB_HOME" in os.environ:
        working_dir = os.environ["MLWB_HOME"]
    else:
        working_dir = os.environ['HOME'] + "/MLWB_HOME"

    if os.path.isdir(working_dir + '/mlwb_dags/'):
        shutil.rmtree(working_dir + '/mlwb_dags/')
    os.makedirs(working_dir + '/mlwb_dags/')
    
    config_loc = normalize_path(execution_dir, vargs['-dag'])
    os.environ['MLWB_CWD'] = get_parent_dir(config_loc)
    shutil.copy2(config_loc, working_dir + '/mlwb_dags')
    
    from airflow.bin.cli import initdb
    from airflow.bin.cli import unpause, clear
    from collections import namedtuple

    initdb({}) # this initiates the wrong db

    Arg = namedtuple(
        'Arg', ['dag_id', 'subdir', 'run_duration', 'num_runs', 'do_pickle', 'pid', 'daemon', 'stdout', 'stderr', 'log_file'])
    Arg.__new__.__defaults__ = (
        None, None, None, 1, False, None, False, None, None, None)

    if "-dag-id" in vargs:
        dag_id = vargs["-dag-id"]
    else:
        learning_config = {}

        # Load learning configuration
        with open(config_loc, 'r') as stream:
            try:
                learning_config = yaml.load(stream)
            except yaml.YAMLError as exc:
                print(exc)
    
        dag_id = learning_config['experiment_name']

    clear_arg = namedtuple('clear_arg',['dag_id', 'task_regex', 'start_date', 'end_date', 'subdir','upstream', 'downstream', 'no_confirm', 'only_failed','only_running', 'exclude_subdags', 'dag_regex'])
    clear_arg.__new__.__defaults__ = (None,False,None,None,None,None,None,True,None,None,None,None)

    scheduler_args = Arg(dag_id=dag_id, subdir=working_dir + '/airflow/dags')
    clear_args = clear_arg(dag_id=dag_id, subdir=working_dir + '/airflow/dags')

    # Start the scheduler
    import subprocess
    import time
    p = subprocess.Popen(["airflow", "scheduler"])
    q = subprocess.Popen(["airflow", "webserver"])
    time.sleep(10)  # Wait for the scheduler to start

    print("=======> Submitting the job....")
    try:
        clear(clear_args)
        unpause(scheduler_args)
        time.sleep(10)  # Wait for the scheduler to trigger dag run
    except:
        print '=======> DAG database creation error'
        p.terminate()
        q.terminate()
        raise

    print("=======> Dag State:" + get_dag_state(dag_id))
    while get_dag_state(dag_id) == "running":
        try:
            print("=======> Dag State:" + get_dag_state(dag_id))
            time.sleep(10)  # Sleep for 10 seconds
        except KeyboardInterrupt:
            print '=======> Keyboard Interrupt'
            p.terminate()
            q.terminate()
            raise

    print("=======> Final Dag State:" + get_dag_state(dag_id))
    print("=======> Shuttindg down the scheduler...")
    p.terminate()
    q.terminate()

if __name__ == '__main__':

    from sys import argv
    vargs = getopts(argv)
    execution_dir = os.getcwd()

    setup_exec_environment(vargs)
    command = argv[1]

    if command == "init":
        init()
    if command == "run":
        init()
        run(vargs, execution_dir)
